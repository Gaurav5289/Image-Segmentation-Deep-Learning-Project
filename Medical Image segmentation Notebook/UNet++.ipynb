{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Project Root: c:\\Users\\hp\\Desktop\\image_Segmentation\\Medical-Image-Segmentation-Deep-Learning-Project\n",
      "âœ… Source Dir added to path: c:\\Users\\hp\\Desktop\\image_Segmentation\\Medical-Image-Segmentation-Deep-Learning-Project\\Code\\source\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from glob import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import OrderedDict\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- 1. ADD YOUR PROJECT'S 'source' FOLDER TO THE PYTHON PATH ---\n",
    "\n",
    "# Get the project root directory (which contains 'Code' and 'Medical Image segmentation Notebook')\n",
    "# os.getcwd() is '.../Medical Image segmentation Notebook'\n",
    "# os.path.abspath(os.path.join(os.getcwd(), \"..\")) is '.../Medical-Image-Segmentation-Deep-Learning-Project'\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "\n",
    "# This is the folder containing all your .py modules (ML_Pipeline, engine.py, etc.)\n",
    "SOURCE_DIR = os.path.join(PROJECT_ROOT, \"Code\", \"source\")\n",
    "\n",
    "# Add this to sys.path so Python can find your modules\n",
    "if SOURCE_DIR not in sys.path:\n",
    "    sys.path.append(SOURCE_DIR)\n",
    "\n",
    "print(f\"âœ… Project Root: {PROJECT_ROOT}\")\n",
    "print(f\"âœ… Source Dir added to path: {SOURCE_DIR}\")\n",
    "\n",
    "\n",
    "# --- 2. IMPORT FROM YOUR .py FILES ---\n",
    "# (Now that the path is set, these imports will work)\n",
    "\n",
    "from ML_Pipeline.dataset import DataSet\n",
    "from ML_Pipeline.network import UNetPP\n",
    "from ML_Pipeline.utils import AverageMeter, iou_score\n",
    "from ML_Pipeline.train import train, validate  # Make sure your train.py has both\n",
    "\n",
    "# --- 3. IMPORT AUGMENTATIONS ---\n",
    "from albumentations import (\n",
    "    Compose, OneOf, RandomRotate90, Flip, HueSaturationValue,\n",
    "    RandomBrightnessContrast, Resize, Normalize\n",
    ")\n",
    "from albumentations.pytorch import ToTensorV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§ Config loaded and paths resolved:\n",
      "  Image Path: c:\\Users\\hp\\Desktop\\image_Segmentation\\Medical-Image-Segmentation-Deep-Learning-Project\\Code/input/PNG/Original\n",
      "  Mask Path: c:\\Users\\hp\\Desktop\\image_Segmentation\\Medical-Image-Segmentation-Deep-Learning-Project\\Code/input/PNG/Ground Truth\n",
      "  Model Path: c:\\Users\\hp\\Desktop\\image_Segmentation\\Medical-Image-Segmentation-Deep-Learning-Project\\Code/output/models/model.pth\n"
     ]
    }
   ],
   "source": [
    "# Load the config file\n",
    "config_path = os.path.join(SOURCE_DIR, \"config.yaml\")\n",
    "with open(config_path, \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# --- Resolve all paths from config ---\n",
    "# (Uses PROJECT_ROOT from Cell 1 to build correct absolute paths)\n",
    "config[\"image_path\"] = os.path.join(PROJECT_ROOT, config[\"image_path\"])\n",
    "config[\"mask_path\"] = os.path.join(PROJECT_ROOT, config[\"mask_path\"])\n",
    "config[\"model_path\"] = os.path.join(PROJECT_ROOT, config[\"model_path\"])\n",
    "config[\"log_path\"] = os.path.join(PROJECT_ROOT, config[\"log_path\"])\n",
    "config[\"output_path\"] = os.path.join(PROJECT_ROOT, config[\"output_path\"])\n",
    "\n",
    "# --- Print to verify ---\n",
    "print(\"ðŸ”§ Config loaded and paths resolved:\")\n",
    "print(f\"  Image Path: {config['image_path']}\")\n",
    "print(f\"  Mask Path: {config['mask_path']}\")\n",
    "print(f\"  Model Path: {config['model_path']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Found 612 images. Split into 489 train, 123 val.\n"
     ]
    }
   ],
   "source": [
    "# Get all image file names (without extension)\n",
    "img_ids = glob(os.path.join(config[\"image_path\"], f\"*{config['extn']}\"))\n",
    "img_ids = [os.path.splitext(os.path.basename(p))[0] for p in img_ids]\n",
    "\n",
    "if not img_ids:\n",
    "    print(f\"âŒ ERROR: No images found at {config['image_path']}\")\n",
    "else:\n",
    "    # Split into train and validation sets\n",
    "    train_img_ids, val_img_ids = train_test_split(img_ids, test_size=0.2, random_state=42)\n",
    "    print(f\"âœ… Found {len(img_ids)} images. Split into {len(train_img_ids)} train, {len(val_img_ids)} val.\")\n",
    "\n",
    "# Define augmentations\n",
    "train_transform = Compose([\n",
    "    RandomRotate90(),\n",
    "    Flip(),\n",
    "    OneOf([\n",
    "        HueSaturationValue(),\n",
    "        RandomBrightnessContrast(),\n",
    "    ], p=1),\n",
    "    Resize(256, 256),\n",
    "    Normalize(),\n",
    "    ToTensorV2(),  # Convert numpy array to torch.Tensor\n",
    "])\n",
    "\n",
    "val_transform = Compose([\n",
    "    Resize(256, 256),\n",
    "    Normalize(),\n",
    "    ToTensorV2(),  # Convert numpy array to torch.Tensor\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… DataLoaders created.\n"
     ]
    }
   ],
   "source": [
    "# Create Dataset objects (using the DataSet class imported from ML_Pipeline/dataset.py)\n",
    "train_dataset = DataSet(\n",
    "    img_ids=train_img_ids,\n",
    "    image_path=config[\"image_path\"],  # Use the absolute path\n",
    "    mask_path=config[\"mask_path\"],    # Use the absolute path\n",
    "    img_ext=config[\"extn\"],\n",
    "    mask_ext=config[\"extn\"],\n",
    "    transform=train_transform\n",
    ")\n",
    "\n",
    "val_dataset = DataSet(\n",
    "    img_ids=val_img_ids,\n",
    "    image_path=config[\"image_path\"],\n",
    "    mask_path=config[\"mask_path\"],\n",
    "    img_ext=config[\"extn\"],\n",
    "    mask_ext=config[\"extn\"],\n",
    "    transform=val_transform\n",
    ")\n",
    "\n",
    "# Create DataLoader objects\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config.get(\"batch_size\", 4),\n",
    "    shuffle=True,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=config.get(\"batch_size\", 4),\n",
    "    shuffle=False,\n",
    "    drop_last=False\n",
    ")\n",
    "\n",
    "print(\"âœ… DataLoaders created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ðŸ› DEBUGGING DATA SHAPES ðŸ› ---\n",
      "Image batch shape: torch.Size([4, 3, 256, 256])\n",
      "Mask batch shape:  torch.Size([4, 256, 256, 1])\n",
      "\n",
      "--- EXPECTED SHAPES ---\n",
      "Image shape should be: torch.Size([4, 3, 256, 256])\n",
      "Mask shape should be:  torch.Size([4, 1, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "print(\"--- ðŸ› DEBUGGING DATA SHAPES ðŸ› ---\")\n",
    "\n",
    "# Get exactly one batch of data from the loader\n",
    "debug_images, debug_masks, _ = next(iter(train_loader))\n",
    "\n",
    "print(f\"Image batch shape: {debug_images.shape}\")\n",
    "print(f\"Mask batch shape:  {debug_masks.shape}\")\n",
    "\n",
    "print(\"\\n--- EXPECTED SHAPES ---\")\n",
    "print(\"Image shape should be: torch.Size([4, 3, 256, 256])\")\n",
    "print(\"Mask shape should be:  torch.Size([4, 1, 256, 256])\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "âœ… Model, Criterion, and Optimizer initialized.\n"
     ]
    }
   ],
   "source": [
    "# Check for GPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Create model (from ML_Pipeline/network.py)\n",
    "model = UNetPP(num_classes=1, deep_supervision=True)\n",
    "model = model.to(device)\n",
    "\n",
    "# Define Loss Function\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Define Optimizer\n",
    "params = filter(lambda p: p.requires_grad, model.parameters())\n",
    "optimizer = optim.Adam(params, lr=config.get(\"learning_rate\", 1e-4), weight_decay=1e-4)\n",
    "\n",
    "print(\"âœ… Model, Criterion, and Optimizer initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Epoch [1/300] ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m--- Epoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Train for one epoch (using imported 'train' function)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m train_log = \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# Validate (using imported 'validate' function)\u001b[39;00m\n\u001b[32m     18\u001b[39m val_log = validate(\u001b[38;5;28;01mTrue\u001b[39;00m, val_loader, model, criterion, device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\Desktop\\image_Segmentation\\Medical-Image-Segmentation-Deep-Learning-Project\\Code\\source\\ML_Pipeline\\train.py:44\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(deep_sup, train_loader, model, criterion, optimizer, device)\u001b[39m\n\u001b[32m     42\u001b[39m \u001b[38;5;66;03m# Backpropagation\u001b[39;00m\n\u001b[32m     43\u001b[39m optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     45\u001b[39m optimizer.step()\n\u001b[32m     47\u001b[39m \u001b[38;5;66;03m# Track metrics\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\anaconda3\\envs\\medseg\\Lib\\site-packages\\torch\\_tensor.py:522\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    512\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    513\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    514\u001b[39m         Tensor.backward,\n\u001b[32m    515\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    520\u001b[39m         inputs=inputs,\n\u001b[32m    521\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m522\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    523\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    524\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\anaconda3\\envs\\medseg\\Lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    261\u001b[39m     retain_graph = create_graph\n\u001b[32m    263\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    264\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    265\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m266\u001b[39m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    267\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    269\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    270\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    271\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    272\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    273\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    274\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "log = OrderedDict([\n",
    "    ('epoch', []), ('loss', []), ('iou', []),\n",
    "    ('val_loss', []), ('val_iou', []),\n",
    "])\n",
    "\n",
    "best_iou = 0\n",
    "trigger = 0\n",
    "epochs = config.get(\"epochs\", 3)  # Get epoch count from config, default to 10\n",
    "patience = config.get(\"early_stopping_patience\", 15)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"\\n--- Epoch [{epoch+1}/{epochs}] ---\")\n",
    "\n",
    "    # Train for one epoch (using imported 'train' function)\n",
    "    train_log = train(True, train_loader, model, criterion, optimizer, device)\n",
    "    \n",
    "    # Validate (using imported 'validate' function)\n",
    "    val_log = validate(True, val_loader, model, criterion, device)\n",
    "\n",
    "    # Print logs\n",
    "    print(f\"Epoch {epoch+1} | Train Loss: {train_log['loss']:.4f} | Train IoU: {train_log['iou']:.4f} | Val Loss: {val_log['loss']:.4f} | Val IoU: {val_log['iou']:.4f}\")\n",
    "\n",
    "    # Update log dictionary\n",
    "    log['epoch'].append(epoch)\n",
    "    log['loss'].append(train_log['loss'])\n",
    "    log['iou'].append(train_log['iou'])\n",
    "    log['val_loss'].append(val_log['loss'])\n",
    "    log['val_iou'].append(val_log['iou'])\n",
    "\n",
    "    # Save log to CSV\n",
    "    pd.DataFrame(log).to_csv(config['log_path'], index=False)\n",
    "\n",
    "    # Early stopping and model checkpointing\n",
    "    trigger += 1\n",
    "    if val_log['iou'] > best_iou:\n",
    "        torch.save(model.state_dict(), config['model_path'])\n",
    "        print(f\"âœ… Model Saved! IoU improved from {best_iou:.4f} to {val_log['iou']:.4f}\")\n",
    "        best_iou = val_log['iou']\n",
    "        trigger = 0\n",
    "    \n",
    "    if trigger >= patience:\n",
    "        print(f\"âš ï¸ Early stopping! No improvement in {patience} epochs.\")\n",
    "        break\n",
    "\n",
    "print(\"\\nðŸŽ‰ Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‚ Current working directory: c:\\Users\\hp\\Desktop\\image_Segmentation\\Medical-Image-Segmentation-Deep-Learning-Project\\Medical Image segmentation Notebook\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- 1. Load the trained model ---\n",
    "# (Make sure to import image_loader from your predict.py file)\n",
    "from ML_Pipeline.predict import image_loader \n",
    "\n",
    "model.load_state_dict(torch.load(config['model_path']))\n",
    "model.eval()\n",
    "print(\"âœ… Best model loaded for prediction.\")\n",
    "\n",
    "# --- 2. Load a test image ---\n",
    "# This path is relative to your PROJECT_ROOT\n",
    "test_img_name = \"115.png\" # You can change this to any image name\n",
    "test_img_path = os.path.join(PROJECT_ROOT, \"Code\", \"input\", \"PNG\", \"Original\", test_img_name)\n",
    "\n",
    "if not os.path.exists(test_img_path):\n",
    "    print(f\"âŒ Test image not found at: {test_img_path}\")\n",
    "else:\n",
    "    # Use the image_loader from predict.py\n",
    "    # It applies Resize, Normalize, and ToTensorV2\n",
    "    image = image_loader(test_img_path, val_transform) # Pass the validation transform\n",
    "    \n",
    "    # Add batch dimension (B, C, H, W) and send to device\n",
    "    image = image.unsqueeze(0).to(device) \n",
    "\n",
    "    # --- 3. Make prediction ---\n",
    "    with torch.no_grad():\n",
    "        output = model(image)\n",
    "        \n",
    "    # Get the last output if deep supervision is on\n",
    "    if isinstance(output, list):\n",
    "        output = output[-1]\n",
    "        \n",
    "    # --- 4. Process and show the mask ---\n",
    "    # Apply sigmoid, remove batch dim, send to CPU, convert to numpy\n",
    "    mask = torch.sigmoid(output).squeeze().cpu().numpy()\n",
    "    \n",
    "    # Convert probabilities (0.0 to 1.0) to a binary mask (0 or 255)\n",
    "    mask_binary = (mask > 0.5).astype(np.uint8) * 255\n",
    "\n",
    "    # --- 5. Show Original, Predicted Mask, and Ground Truth ---\n",
    "    gt_mask_path = os.path.join(PROJECT_ROOT, \"Code\", \"input\", \"PNG\", \"Ground Truth\", test_img_name)\n",
    "    gt_mask = cv2.imread(gt_mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "    original_image = cv2.imread(test_img_path)\n",
    "    original_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB) # for plt\n",
    "\n",
    "    fig, ax = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    ax[0].imshow(original_image)\n",
    "    ax[0].set_title(\"Original Image\")\n",
    "    ax[0].axis(\"off\")\n",
    "    \n",
    "    ax[1].imshow(mask_binary, cmap=\"gray\")\n",
    "    ax[1].set_title(\"Predicted Mask\")\n",
    "    ax[1].axis(\"off\")\n",
    "    \n",
    "    ax[2].imshow(gt_mask, cmap=\"gray\")\n",
    "    ax[2].set_title(\"Ground Truth\")\n",
    "    ax[2].axis(\"off\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # Save the prediction image\n",
    "    cv2.imwrite(config['output_path'], mask_binary)\n",
    "    print(f\"âœ… Prediction saved to: {config['output_path']}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "UNET++.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "medseg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
